{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deu.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "train_sentences = []\n",
    "target_sentences = []\n",
    "train_char = set()\n",
    "target_char = set()\n",
    "samples = 150000\n",
    "# samples = 1000\n",
    "for line in lines[: min(samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    train_sentences.append(input_text)\n",
    "    target_sentences.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in train_char:\n",
    "            train_char.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_char:\n",
    "            target_char.add(char)\n",
    "\n",
    "print(train_sentences[99000])\n",
    "print(target_sentences[99000])\n",
    "\n",
    "train_char = sorted(list(train_char))\n",
    "target_char = sorted(list(target_char))\n",
    "encoder_token_len = len(train_char)\n",
    "decoder_token_len = len(target_char)\n",
    "total_length_encoder = max([len(txt) for txt in train_sentences])\n",
    "total_length_decoder = max([len(txt) for txt in target_sentences])\n",
    "\n",
    "print('Number of samples:', len(train_sentences))\n",
    "print('Number of unique input tokens:', encoder_token_len)\n",
    "print('Number of unique output tokens:', decoder_token_len)\n",
    "print('Max sequence length for inputs:', total_length_encoder)\n",
    "print('Max sequence length for outputs:', total_length_decoder)\n",
    "\n",
    "\n",
    "tokenized_encoder_intvalue = dict(\n",
    "  [(char, i) for i, char in enumerate(train_char)])\n",
    "tokenized_decoder_intvalue = dict(\n",
    "  [(char, i) for i, char in enumerate(target_char)])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "En_input = np.zeros(\n",
    "  (len(train_sentences), total_length_encoder, encoder_token_len),\n",
    "  dtype='float32')\n",
    "De_input = np.zeros(\n",
    "  (len(train_sentences), total_length_decoder, decoder_token_len),\n",
    "  dtype='float32')\n",
    "De_output = np.zeros(\n",
    "  (len(train_sentences), total_length_decoder, decoder_token_len),\n",
    "  dtype='float32')\n",
    "\n",
    "print(En_input.shape)\n",
    "print(De_input.shape)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(train_sentences, target_sentences)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        En_input[i, t, tokenized_encoder_intvalue[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # De_output is ahead of De_input by one timestep\n",
    "        De_input[i, t, tokenized_decoder_intvalue[char]] = 1.\n",
    "        if t > 0:\n",
    "            # De_output will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            De_output[i, t - 1, tokenized_decoder_intvalue[char]] = 1.\n",
    "\n",
    "import keras, tensorflow\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # batch size for training\n",
    "epochs = 100  # number of epochs to train for\n",
    "latent_dim = 256  # latent dimensionality of the encoding space\n",
    "\n",
    "input_encoder = Input(shape=(None, encoder_token_len))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "output_encoder, state_h, state_c = encoder(input_encoder)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "input_decoder = Input(shape=(None, decoder_token_len))\n",
    "decoder_lstm_model = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "output_decoder, _, _ = decoder_lstm_model(input_decoder,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense_layer = Dense(decoder_token_len, activation='softmax')\n",
    "output_decoder = decoder_dense_layer(output_decoder)\n",
    "\n",
    "model = Model(inputs=[input_encoder, input_decoder],\n",
    "              outputs=output_decoder)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "model.fit([En_input, De_input], De_output,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           validation_split=0.2)\n",
    "model.save('abcd.h5')  #change the file to complete.h5 to get the trained weights\n",
    "\n",
    "\n",
    "model = Model([input_encoder, input_decoder], output_decoder)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.load_weights(\"abcd.h5\")\n",
    "\n",
    "encoder_model = Model(input_encoder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "output_decoder, state_h, state_c = decoder_lstm_model(\n",
    "  input_decoder, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "output_decoder = decoder_dense_layer(output_decoder)\n",
    "\n",
    "decoder_model = Model(\n",
    "  [input_decoder] + decoder_states_inputs,\n",
    "  [output_decoder] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "  (i, char) for char, i in tokenized_encoder_intvalue.items())\n",
    "reverse_target_char_index = dict(\n",
    "  (i, char) for char, i in tokenized_decoder_intvalue.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # encode the input sequence to get the internal state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # generate empty target sequence of length 1 with only the start character\n",
    "    target_seq = np.zeros((1, 1, decoder_token_len))\n",
    "    target_seq[0, 0, tokenized_decoder_intvalue['\\t']] = 1.\n",
    "\n",
    "    # output sequence loop\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # sample a token and add the corresponding character to the\n",
    "        # decoded sequence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # check for the exit condition: either hitting max length\n",
    "        # or predicting the 'stop' character\n",
    "        if (sampled_char == '\\n' or\n",
    "                len(decoded_sentence) > total_length_decoder):\n",
    "            stop_condition = True\n",
    "\n",
    "        # update the target sequence (length 1).\n",
    "        target_seq = np.zeros((1, 1, decoder_token_len))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "    input_seq = En_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', train_sentences[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "\n",
    "\"\"\" enter the sentence that needs to be translated\"\"\"\n",
    "input_sentence = \"i do not like this movie at all?\"\n",
    "test_sentence_tokenized = np.zeros(\n",
    "  (1, total_length_encoder, encoder_token_len), dtype='float32')\n",
    "for t, char in enumerate(input_sentence):\n",
    "    test_sentence_tokenized[0, t, tokenized_encoder_intvalue[char]] = 1.\n",
    "print(input_sentence)\n",
    "print(decode_sequence(test_sentence_tokenized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
